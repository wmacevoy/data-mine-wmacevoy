# Supervised Learning: Regression & Classification

Supervised learning maps input features to known outputs.  We start with **regression**, covering ordinary least squares and the geometry of projections.  To combat overfitting, regularization techniques—Ridge (L2) and Lasso (L1)—are explained and visualized through coefficient paths.  Students implement polynomial and interaction features and use cross‑validation to tune hyperparameters.

For **classification**, we introduce logistic regression and discuss the interpretation of log‑odds.  Instance‑based methods like k‑Nearest Neighbors illustrate the importance of distance metrics and feature scaling, while Naïve Bayes offers a probabilistic baseline that works surprisingly well for text.  We examine confusion matrices, ROC and precision–recall curves, and discuss threshold selection and class imbalance remedies such as resampling and class weights.  The goal is to build interpretable baselines before moving to more complex models.
