# Margins & Neural Networks

Support Vector Machines (SVMs) classify data by finding hyperplanes with maximal margin, offering robustness to outliers.  We derive the primal and dual forms for linear SVMs and illustrate how slack variables handle non‑separable data.  Kernel functions (polynomial, radial basis function) implicitly map inputs into higher‑dimensional spaces, allowing SVMs to fit complex boundaries.  Students learn when SVMs are effective and how to tune C and gamma parameters.

We then move from single‑layer perceptrons to shallow **multi‑layer perceptrons** (MLPs).  Building on knowledge of linear algebra and calculus, we explain feed‑forward computation, activation functions (ReLU, sigmoid, softmax) and back‑propagation for training.  Regularization (dropout, weight decay) and early stopping prevent overfitting.  Although deep neural networks are beyond scope, this module demystifies the basics and illustrates that neural nets generalize logistic regression.
