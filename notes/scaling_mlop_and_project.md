# Scaling, MLOps & Final Project

As datasets and models grow, scalable tooling becomes essential.  We demonstrate how to refactor notebook code into modular pipelines, separating data loading, transformation, modeling and evaluation.  Tools like *DVC* and *MLflow* track data and model versions.  Continuous integration and continuous deployment (CI/CD) practices ensure code quality and reproducibility.

Students explore distributed computing frameworks **Spark** and **Dask**, understanding how parallelism and lazy evaluation handle large datasets.  We briefly discuss streaming data and concept drift, highlighting the need for model monitoring, drift detection and automated retraining triggers.

The culmination of the course is the team project.  Teams deliver a reproducible end‑to‑end analysis on a real dataset, documenting their process in a short report and presenting insights to classmates.  Grading emphasizes methodological soundness, reproducibility, ethical reflection and clarity of communication.  This project synthesizes skills from every module and encourages independent exploration.
